{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Universality Verification Experiment for Scaling Law (D-Scaling)\n",
    "\n",
    "## Core Theory and Functionality\n",
    "This version represents the final presentation of the theory. Building upon v12, it restores the classic performance scaling plot to form a complete narrative chain:\n",
    "\n",
    "1.  **Restored Performance Plot (Law 1)**: The \"Performance vs. Data Size\" plot is reinstated as the first figure, intuitively demonstrating the direct connection of the experiment to Scaling Law research.\n",
    "2.  **Complete Theoretical Narrative**: The final chart comprises 5 parts, holistically presenting the entire unified theory—from macroscopic performance, to the decomposition of internal components, and finally to the core predictive law.\n",
    "3.  **Data Persistence**: The functionality to save experimental results to a CSV file has been retained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset on the first run\n",
    "from torchvision import datasets, transforms\n",
    "try:\n",
    "    datasets.FashionMNIST('./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "    datasets.FashionMNIST('./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "    print(\"FashionMNIST dataset is ready.\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not download dataset. Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the experiment's directory to the Python path\n",
    "workspace_path = \"/D-Scaling\" # Please adjust this path according to your setup\n",
    "if workspace_path not in sys.path:\n",
    "    sys.path.append(workspace_path)\n",
    "    print(f\"Added path: {workspace_path}\")\n",
    "else:\n",
    "    print(f\"Path {workspace_path} is already in the Python path\")\n",
    "\n",
    "# Confirm the existence of the logic module file\n",
    "file_path = os.path.join(workspace_path, \"MLP_D_logic.py\") # Find and replace with the logic module for the desired model\n",
    "if os.path.exists(file_path):\n",
    "    print(f\"✓ File exists: {file_path}\")\n",
    "else:\n",
    "    print(f\"✗ File not found: {file_path}\")\n",
    "\n",
    "# Attempt to import the module\n",
    "try:\n",
    "    import MLP_D_logic # Replace with the logic module for the desired model\n",
    "    print(\"✓ Module imported successfully!\")\n",
    "    \n",
    "    # Check for the presence of the necessary function\n",
    "    if hasattr(MLP_D_logic, 'run_training_task'):\n",
    "        print(\"✓ Found function: run_training_task\")\n",
    "        run_training_task = MLP_D_logic.run_training_task\n",
    "    else:\n",
    "        print(\"✗ Function not found: run_training_task\")\n",
    "        \n",
    "except ImportError as e:\n",
    "    print(f\"✗ Import failed: {e}\")\n",
    "    print(\"Attempting alternative import method...\")\n",
    "    \n",
    "    # Use importlib as a fallback\n",
    "    try:\n",
    "        import importlib.util\n",
    "        spec = importlib.util.spec_from_file_location(\"MLP_D_logic\", file_path)\n",
    "        MLP_D_logic = importlib.util.module_from_spec(spec)\n",
    "        spec.loader.exec_module(MLP_D_logic)\n",
    "        print(\"✓ Successfully imported using importlib\")\n",
    "\n",
    "        if hasattr(MLP_D_logic, 'run_training_task'):\n",
    "            print(\"✓ Found function: run_training_task\")\n",
    "            run_training_task = MLP_D_logic.run_training_task\n",
    "        else:\n",
    "            print(\"✗ Function not found: run_training_task\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Alternative import also failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "single_run_config_cell"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# --- 1. Single Run Configuration ---\n",
    "SINGLE_RUN_CONFIG = {\n",
    "    \"seed\": 42,\n",
    "    \"epochs\": 60,\n",
    "}\n",
    "\n",
    "# --- 2. Base Experiment Parameters ---\n",
    "BASE_CONFIG = {\n",
    "    \"d_values\": np.logspace(2, 4.7, 25).astype(int),\n",
    "    \"num_points_for_linf_estimation\": 5,\n",
    "    \"batch_size\": 128,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"analysis_sample_size\": 30,\n",
    "    \"w1\": 0.5, \n",
    "    \"w2\": 0.5,\n",
    "}\n",
    "BASE_CONFIG['epochs'] = SINGLE_RUN_CONFIG['epochs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "main_notebook_cell_v13"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import linregress\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from MLP_D_logic import run_training_task, TheoryAnalyzer # Replace with the logic module for the desired model\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    seed = SINGLE_RUN_CONFIG['seed']\n",
    "    d_values = BASE_CONFIG['d_values']\n",
    "    tasks = [(seed, d, BASE_CONFIG, 0 if torch.cuda.is_available() else -1) for d in d_values]\n",
    "    \n",
    "    results = []\n",
    "    for task_args in tqdm(tasks, desc=f\"Running experiment for Seed {seed}, Epochs {BASE_CONFIG['epochs']}\"):\n",
    "        result = run_training_task(task_args)\n",
    "        if result: results.append(result)\n",
    "    \n",
    "    if not results:\n",
    "        print(\"No results generated.\")\n",
    "    else:\n",
    "        df = pd.DataFrame(results).sort_values('data_size_d')\n",
    "        \n",
    "        plt.style.use('seaborn-v0_8-whitegrid')\n",
    "        fig, axes = plt.subplots(1, 5, figsize=(40, 8))\n",
    "        fig.suptitle(f'Cognitive Investment Model - Unified Theory (Seed={seed}, Epochs={BASE_CONFIG[\"epochs\"]})', fontsize=24, y=0.98)\n",
    "        \n",
    "        # --- Fitting Functions ---\n",
    "        def power_law_fit(x, y):\n",
    "            mask = (y > 0) & (x > 0) & np.isfinite(y) & np.isfinite(x)\n",
    "            if mask.sum() < 2: return 0, 1, 0, np.full_like(x, np.nan, dtype=float)\n",
    "            lx, ly = np.log10(x[mask]), np.log10(y[mask])\n",
    "            s, i, r, p, _ = linregress(lx, ly); r2 = r**2\n",
    "            return r2, p, s, 10**(s*np.log10(x)+i)\n",
    "            \n",
    "        def exp_decay_fit(x, y):\n",
    "            mask = (y > 0) & np.isfinite(y) & np.isfinite(x)\n",
    "            if mask.sum() < 2: return 0, 1, 0, np.full_like(x, np.nan, dtype=float)\n",
    "            lx, ly = x[mask], np.log(y[mask])\n",
    "            s, i, r, p, _ = linregress(lx, ly); r2 = r**2\n",
    "            return r2, p, -s, np.exp(s*x+i)\n",
    "\n",
    "        # --- Data Preparation for Reducible Metrics ---\n",
    "        n_est = BASE_CONFIG['num_points_for_linf_estimation']\n",
    "        \n",
    "        L_inf = df['final_test_loss'].tail(n_est).mean()\n",
    "        df['reducible_loss'] = df['final_test_loss'] - L_inf\n",
    "        hsie_inf = df['final_hsie'].tail(n_est).mean()\n",
    "        df['reducible_hsie'] = df['final_hsie'] - hsie_inf\n",
    "        \n",
    "        htse_0 = df['final_htse'].head(n_est).mean()\n",
    "        df['reducible_htse'] = df['final_htse'] - htse_0\n",
    "\n",
    "        x_d = df['data_size_d'].values\n",
    "        \n",
    "        # --- [Plot 1 - Restored] Performance Scaling Law ---\n",
    "        ax0 = axes[0]\n",
    "        r2_1, p_1, s_1, fit_1 = power_law_fit(x_d, df['reducible_loss'])\n",
    "        ax0.set_title('Law 1: Performance vs. Data (Power Law)')\n",
    "        ax0.plot(x_d, df['final_test_loss'], 'o', color='blue')\n",
    "        ax0.plot(x_d, fit_1 + L_inf, '--', color='red')\n",
    "        ax0.text(0.95, 0.95, f'$R^2={r2_1:.2f}, p={p_1:.1e}$\\n$L-L_\\infty \\propto D^{{{s_1:.2f}}}$', ha='right', va='top', transform=ax0.transAxes, bbox=dict(fc='wheat', alpha=0.5))\n",
    "        ax0.set_ylabel('Final Test Loss')\n",
    "\n",
    "        # --- [Plot 2] Component 1: Abstraction ---\n",
    "        r2_2, p_2, s_2, fit_2 = power_law_fit(x_d, df['reducible_htse'])\n",
    "        axes[1].set_title('Component 1: Abstraction (Power Law)');\n",
    "        axes[1].plot(x_d, df['final_htse'], 's', color='green')\n",
    "        axes[1].plot(x_d, fit_2 + htse_0, '--', color='purple')\n",
    "        axes[1].text(0.95, 0.05, f'$R^2={r2_2:.2f}, p={p_2:.1e}$\\n$H-H_0 \\propto D^{{{s_2:.2f}}}$', ha='right', va='bottom', transform=axes[1].transAxes, bbox=dict(fc='wheat', alpha=0.5))\n",
    "        axes[1].set_ylabel(\"Final H'_TSE\")\n",
    "        \n",
    "        # --- [Plot 3] Component 2: Compression ---\n",
    "        r2_3, p_3, d_rate_3, fit_3 = exp_decay_fit(x_d, df['reducible_hsie'])\n",
    "        axes[2].set_title('Component 2: Compression (Exp. Decay)')\n",
    "        axes[2].plot(x_d, df['final_hsie'], '^', color='orange')\n",
    "        axes[2].plot(x_d, fit_3 + hsie_inf, '--', color='darkred')\n",
    "        axes[2].text(0.95, 0.95, f'$R^2={r2_3:.2f}, p={p_3:.1e}$\\n$H-H_\\infty \\propto e^{{-{d_rate_3:.1e}D}}$', ha='right', va='top', transform=axes[2].transAxes, bbox=dict(fc='wheat', alpha=0.5))\n",
    "        axes[2].set_ylabel(\"Final H'_SIE\")\n",
    "        \n",
    "        # --- [Plot 4] Internal Cost Trend ---\n",
    "        htse_red_sq = np.maximum(0, df['reducible_htse'])**2\n",
    "        hsie_red_sq = np.maximum(0, df['reducible_hsie'])**2\n",
    "        df['L_ideal_reducible'] = np.sqrt(BASE_CONFIG['w1'] * htse_red_sq + BASE_CONFIG['w2'] * hsie_red_sq)\n",
    "        axes[3].plot(x_d, df['L_ideal_reducible'], 'p', color='purple')\n",
    "        axes[3].set_title('Internal Cost $\\mathcal{L}_{ideal, red}$ Trend')\n",
    "        axes[3].set_ylabel('Reducible Ideal Norm')\n",
    "\n",
    "        # --- [Plot 5] The Core Law: Performance vs. Cost ---\n",
    "        ax4 = axes[4]\n",
    "        ax4.set_title('The Core Law: Performance vs. Cost', fontsize=16)\n",
    "        r2_4, p_4, s_4, fit_4 = power_law_fit(df['L_ideal_reducible'], df['reducible_loss'])\n",
    "        ax4.plot(df['L_ideal_reducible'], df['reducible_loss'], 'd', color='black')\n",
    "        ax4.plot(df['L_ideal_reducible'], fit_4, '--', color='magenta', lw=2.5)\n",
    "        core_text = (f'Core Predictive Law:\\n' \n",
    "                     f'$R^2 = {r2_4:.4f}$ ($p={p_4:.1e}$)\\n'\n",
    "                     f'$L_{{red}} \\propto \\mathcal{{L}}_{{ideal, red}}^{{{s_4:.2f}}}$')\n",
    "        ax4.text(0.95, 0.95, core_text, transform=ax4.transAxes, fontsize=12, va='top', ha='right', bbox=dict(fc='wheat', alpha=0.5))\n",
    "        ax4.set_xlabel('Reducible Ideal Norm $\\mathcal{L}_{ideal, red}$', fontsize=12)\n",
    "        ax4.set_ylabel('Reducible Test Loss', fontsize=12)\n",
    "\n",
    "        # --- Final Formatting ---\n",
    "        for i in range(5):\n",
    "            if i < 4:\n",
    "                axes[i].set_xlabel('Dataset Size D')\n",
    "            axes[i].set_xscale('log')\n",
    "            axes[i].set_yscale('log')\n",
    "            axes[i].grid(True, which='both', linestyle='--')\n",
    "        \n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        \n",
    "        # --- Save results ---\n",
    "        output_img_path = f\"scaling_law_unified_theory_seed_{seed}_epochs_{BASE_CONFIG['epochs']}.png\"\n",
    "        output_csv_path = f\"results_seed_{seed}_epochs_{BASE_CONFIG['epochs']}.csv\"\n",
    "        \n",
    "        plt.savefig(output_img_path, dpi=150)\n",
    "        df.to_csv(output_csv_path, index=False)\n",
    "        \n",
    "        print(f\"Plot saved to: {output_img_path}\")\n",
    "        print(f\"Experiment data saved to: {output_csv_path}\")\n",
    "        \n",
    "        plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
