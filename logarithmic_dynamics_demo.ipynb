{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*Please run this script before experiment,and you will be surprised about the result.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BU42faNOsoJ2",
        "outputId": "7f900694-a8fd-468c-af1a-d96b3f6288fa"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_moons\n",
        "from tqdm import trange\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import networkx as nx\n",
        "import copy\n",
        "import warnings\n",
        "from scipy.optimize import curve_fit\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "# Suppress FutureWarning from seaborn/numpy\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "# --- 1. Configuration Parameters ---\n",
        "N_SAMPLES = 2000\n",
        "N_EPOCHS = 5000\n",
        "BATCH_SIZE = 128\n",
        "LEARNING_RATE = 0.05\n",
        "WEIGHT_DECAY = 0 # You can adjust this L2 regularization strength\n",
        "N_BINS = 30  # Number of bins for MI calculation\n",
        "ANALYSIS_SAMPLE_SIZE = 30 # For H'tse/H'sie calculation\n",
        "LOG_INTERVAL = 10 # How often to log MI and Entropy metrics\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "print(f\"Weight Decay (L2 Regularization) set to: {WEIGHT_DECAY}\")\n",
        "\n",
        "# --- 2. Define the MLP Model ---\n",
        "class SimpleMLP(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, output_dim):\n",
        "        super(SimpleMLP, self).__init__()\n",
        "        # Wrap layers in a Sequential container to make them easily iterable for the TheoryAnalyzer\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim1),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim1, hidden_dim2),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim2, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, return_hidden=False):\n",
        "        h1, h2 = None, None\n",
        "        # Manually pass through layers to capture hidden activations\n",
        "        x = self.layers[0](x)\n",
        "        x = self.layers[1](x)\n",
        "        h1 = x\n",
        "        x = self.layers[2](x)\n",
        "        x = self.layers[3](x)\n",
        "        h2 = x\n",
        "        out = self.layers[4](x)\n",
        "\n",
        "        if return_hidden:\n",
        "            return out, h1, h2\n",
        "        return out\n",
        "\n",
        "# --- 3. Prepare Data ---\n",
        "X, y = make_moons(n_samples=N_SAMPLES, noise=0.1, random_state=42)\n",
        "X = torch.FloatTensor(X).to(DEVICE)\n",
        "y = torch.LongTensor(y).to(DEVICE)\n",
        "\n",
        "train_dataset = torch.utils.data.TensorDataset(X, y)\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "\n",
        "# --- 4. Information Theory & Unified Theory Analysis Tools ---\n",
        "\n",
        "# 4.1 Mutual Information Calculation Functions\n",
        "def calculate_mutual_information(x, y, bins):\n",
        "    \"\"\"Calculates the mutual information I(X;Y) between two variables.\"\"\"\n",
        "    joint_hist, _, _ = np.histogram2d(x, y, bins=bins)\n",
        "    joint_prob = joint_hist / np.sum(joint_hist)\n",
        "    p_x = np.sum(joint_prob, axis=1)\n",
        "    p_y = np.sum(joint_prob, axis=0)\n",
        "    mi = 0.0\n",
        "    for i in range(bins):\n",
        "        for j in range(bins):\n",
        "            if joint_prob[i, j] > 1e-12 and p_x[i] > 1e-12 and p_y[j] > 1e-12:\n",
        "                mi += joint_prob[i, j] * np.log2(joint_prob[i, j] / (p_x[i] * p_y[j]))\n",
        "    return mi\n",
        "\n",
        "def get_mi_for_layer(model, data_loader, layer_idx, n_bins):\n",
        "    \"\"\"Gets the mutual information for a specified hidden layer.\"\"\"\n",
        "    model.eval()\n",
        "    all_inputs = data_loader.dataset.tensors[0].cpu().numpy()\n",
        "    all_labels = data_loader.dataset.tensors[1].cpu().numpy()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        _, h1, h2 = model(data_loader.dataset.tensors[0], return_hidden=True)\n",
        "\n",
        "    hidden_activations = h1 if layer_idx == 1 else h2\n",
        "    hidden_activations = hidden_activations.cpu().numpy()\n",
        "\n",
        "    digitized_input = np.digitize(all_inputs[:, 0], bins=np.linspace(np.min(all_inputs[:, 0]), np.max(all_inputs[:, 0]), n_bins))\n",
        "    digitized_hidden = np.digitize(hidden_activations[:, 0], bins=np.linspace(np.min(hidden_activations[:, 0]), np.max(hidden_activations[:, 0]), n_bins))\n",
        "    digitized_labels = all_labels\n",
        "\n",
        "    mi_xt = calculate_mutual_information(digitized_input, digitized_hidden, n_bins)\n",
        "    mi_ty = calculate_mutual_information(digitized_hidden, digitized_labels, n_bins)\n",
        "    return mi_xt, mi_ty\n",
        "\n",
        "# 4.2 Unified Theory Analyzer (H'_tse and H'_sie) from your script\n",
        "class TheoryAnalyzer:\n",
        "    def __init__(self, model):\n",
        "        model_copy = copy.deepcopy(model)\n",
        "        model_copy.eval()\n",
        "        self.model = model_copy.to('cpu')\n",
        "        self.graph = self._build_graph()\n",
        "        self.hidden_nodes = self._get_hidden_nodes()\n",
        "        self.memoized_paths = {}\n",
        "\n",
        "    def _build_graph(self):\n",
        "        G = nx.DiGraph()\n",
        "        node_counter = 0; layer_map = {}\n",
        "        linear_layers = [l for l in self.model.layers if isinstance(l, nn.Linear)]\n",
        "        if not linear_layers: return G\n",
        "        in_features = linear_layers[0].in_features\n",
        "        layer_map[0] = list(range(node_counter, node_counter + in_features))\n",
        "        for i in range(in_features):\n",
        "            G.add_node(node_counter, layer=0); node_counter += 1\n",
        "        graph_layer_idx = 1\n",
        "        for l in linear_layers:\n",
        "            layer_map[graph_layer_idx] = list(range(node_counter, node_counter + l.out_features))\n",
        "            for i in range(l.out_features):\n",
        "                G.add_node(node_counter, layer=graph_layer_idx); node_counter += 1\n",
        "            weights = torch.abs(l.weight.data.t()); probs = torch.softmax(weights, dim=1)\n",
        "            for u_local_idx, u_global_idx in enumerate(layer_map[graph_layer_idx - 1]):\n",
        "                for v_local_idx, v_global_idx in enumerate(layer_map[graph_layer_idx]):\n",
        "                    prob = probs[u_local_idx, v_local_idx].item()\n",
        "                    if prob > 1e-9: G.add_edge(u_global_idx, v_global_idx, cost=1.0 - np.log(prob + 1e-9))\n",
        "            graph_layer_idx += 1\n",
        "        self.grounding_nodes = set(layer_map.get(graph_layer_idx - 1, [])); return G\n",
        "\n",
        "    def _get_hidden_nodes(self):\n",
        "        max_layer_idx = max((data['layer'] for _, data in self.graph.nodes(data=True)), default=0)\n",
        "        return [node for node, data in self.graph.nodes(data=True) if data['layer'] not in [0, max_layer_idx]]\n",
        "\n",
        "    def find_all_paths_dfs(self, start, targets):\n",
        "        memo_key = (start, tuple(sorted(list(targets))))\n",
        "        if memo_key in self.memoized_paths: return self.memoized_paths[memo_key]\n",
        "        paths, stack = [], [(start, [start], 0)]\n",
        "        while stack:\n",
        "            curr, path, cost = stack.pop()\n",
        "            if curr in targets: paths.append({'path': path, 'cost': cost}); continue\n",
        "            if len(path) > 10: continue\n",
        "            for neighbor in self.graph.neighbors(curr):\n",
        "                edge_cost = self.graph.get_edge_data(curr, neighbor, {}).get('cost', float('inf'))\n",
        "                if neighbor not in path: stack.append((neighbor, path + [neighbor], cost + edge_cost))\n",
        "        self.memoized_paths[memo_key] = paths; return paths\n",
        "\n",
        "    def calculate_metrics_for_node(self, node):\n",
        "        paths = self.find_all_paths_dfs(node, self.grounding_nodes)\n",
        "        if not paths: return float('inf'), 0.0\n",
        "        costs = np.array([p['cost'] for p in paths])\n",
        "        conductances = 1.0 / costs\n",
        "        htse = 1.0 / np.sum(conductances) if np.sum(conductances) > 0 else float('inf')\n",
        "        importances = np.exp(-1.0 * costs)\n",
        "        probabilities = importances / np.sum(importances) if np.sum(importances) > 0 else np.zeros_like(importances)\n",
        "        hsie = -np.sum(probabilities * np.log2(probabilities + 1e-9))\n",
        "        return htse, hsie\n",
        "\n",
        "    def analyze_model_structure(self, analysis_sample_size):\n",
        "        htse_vals, hsie_vals = [], []\n",
        "        if not self.hidden_nodes: return 0, 0\n",
        "        sample_size = min(analysis_sample_size, len(self.hidden_nodes))\n",
        "        sampled_nodes = np.random.choice(self.hidden_nodes, size=sample_size, replace=False)\n",
        "        for node in sampled_nodes:\n",
        "            htse, hsie = self.calculate_metrics_for_node(node)\n",
        "            if np.isfinite(htse) and np.isfinite(hsie): htse_vals.append(htse); hsie_vals.append(hsie)\n",
        "        return np.mean(htse_vals) if htse_vals else 0, np.mean(hsie_vals) if hsie_vals else 0\n",
        "\n",
        "\n",
        "# --- 5. Train Model and Log Metrics ---\n",
        "model = SimpleMLP(input_dim=2, hidden_dim1=10, hidden_dim2=7, output_dim=2).to(DEVICE)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "history = {\n",
        "    'loss': [], 'mi_xt_h1': [], 'mi_ty_h1': [],\n",
        "    'htse': [], 'hsie': []\n",
        "}\n",
        "\n",
        "pbar = trange(N_EPOCHS, desc=\"Training\")\n",
        "for epoch in pbar:\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for batch_x, batch_y in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_x)\n",
        "        loss = criterion(outputs, batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    avg_loss = epoch_loss / len(train_loader)\n",
        "    history['loss'].append(avg_loss)\n",
        "\n",
        "    if epoch % LOG_INTERVAL == 0:\n",
        "        # Calculate IB metrics\n",
        "        mi_xt1, mi_ty1 = get_mi_for_layer(model, train_loader, layer_idx=1, n_bins=N_BINS)\n",
        "        history['mi_xt_h1'].append(mi_xt1); history['mi_ty_h1'].append(mi_ty1)\n",
        "\n",
        "        # Calculate Unified Theory metrics\n",
        "        analyzer = TheoryAnalyzer(model)\n",
        "        htse, hsie = analyzer.analyze_model_structure(ANALYSIS_SAMPLE_SIZE)\n",
        "        history['htse'].append(htse); history['hsie'].append(hsie)\n",
        "\n",
        "        pbar.set_postfix_str(f\"Loss:{avg_loss:.3f}, I(X;T):{mi_xt1:.2f}, I(T;Y):{mi_ty1:.2f}, H'tse:{htse:.3f}, H'sie:{hsie:.3f}\")\n",
        "    else:\n",
        "        for key in history:\n",
        "            if key != 'loss': history[key].append(float('nan'))\n",
        "\n",
        "# --- 6. Curve Fitting and Statistical Analysis ---\n",
        "\n",
        "# Define fitting functions based on your logarithmic hypothesis\n",
        "def log_growth_func(x, a, b):\n",
        "    \"\"\"Logarithmic growth function: y = a * log(x + 1) + b\"\"\"\n",
        "    return a * np.log(x + 1) + b\n",
        "\n",
        "def log_decay_func(x, a, b):\n",
        "    \"\"\"Logarithmic decay function: y = -a * log(x + 1) + b\"\"\"\n",
        "    return -a * np.log(x + 1) + b\n",
        "\n",
        "# Clean up NaN values for plotting and fitting\n",
        "log_epochs_np = np.array([i for i, val in enumerate(history['mi_xt_h1']) if not np.isnan(val)])\n",
        "mi_xt_h1 = np.array([val for val in history['mi_xt_h1'] if not np.isnan(val)])\n",
        "mi_ty_h1 = np.array([val for val in history['mi_ty_h1'] if not np.isnan(val)])\n",
        "htse_hist = np.array([val for val in history['htse'] if not np.isnan(val)])\n",
        "hsie_hist = np.array([val for val in history['hsie'] if not np.isnan(val)])\n",
        "\n",
        "# Fit H'_tse (Cognitive Cost)\n",
        "try:\n",
        "    params_htse, _ = curve_fit(log_growth_func, log_epochs_np, htse_hist, p0=[0.001, 0.54], maxfev=5000)\n",
        "    y_fit_htse = log_growth_func(log_epochs_np, *params_htse)\n",
        "    residuals_htse = htse_hist - y_fit_htse\n",
        "    ss_res_htse = np.sum(residuals_htse**2)\n",
        "    ss_tot_htse = np.sum((htse_hist - np.mean(htse_hist))**2)\n",
        "    r2_htse = 1 - (ss_res_htse / ss_tot_htse)\n",
        "    r_htse, p_htse = pearsonr(htse_hist, y_fit_htse)\n",
        "\n",
        "    # NEW: Print the results to the console\n",
        "    print(\"\\n--- H'_tse (Cognitive Cost) Curve Fit ---\")\n",
        "    print(f\"Fit function: y = a * ln(x + 1) + b\")\n",
        "    print(f\"  - Parameter a: {params_htse[0]:.6f}\")\n",
        "    print(f\"  - Parameter b: {params_htse[1]:.6f}\")\n",
        "    print(f\"R-squared: {r2_htse:.4f}\")\n",
        "    print(f\"p-value: {p_htse:.2e}\")\n",
        "\n",
        "except RuntimeError:\n",
        "    print(\"Could not fit H'_tse curve.\")\n",
        "    y_fit_htse, r2_htse, p_htse, params_htse = None, 0, 1, [0,0]\n",
        "\n",
        "# Fit H'_sie (Robustness)\n",
        "try:\n",
        "    params_hsie, _ = curve_fit(log_decay_func, log_epochs_np, hsie_hist, p0=[0.01, 2.7], maxfev=5000)\n",
        "    y_fit_hsie = log_decay_func(log_epochs_np, *params_hsie)\n",
        "    residuals_hsie = hsie_hist - y_fit_hsie\n",
        "    ss_res_hsie = np.sum(residuals_hsie**2)\n",
        "    ss_tot_hsie = np.sum((hsie_hist - np.mean(hsie_hist))**2)\n",
        "    r2_hsie = 1 - (ss_res_hsie / ss_tot_hsie)\n",
        "    r_hsie, p_hsie = pearsonr(hsie_hist, y_fit_hsie)\n",
        "\n",
        "    # NEW: Print the results to the console\n",
        "    print(\"\\n--- H'_sie (Robustness) Curve Fit ---\")\n",
        "    print(f\"Fit function: y = -a * ln(x + 1) + b\")\n",
        "    print(f\"  - Parameter a: {params_hsie[0]:.6f}\")\n",
        "    print(f\"  - Parameter b: {params_hsie[1]:.6f}\")\n",
        "    print(f\"R-squared: {r2_hsie:.4f}\")\n",
        "    print(f\"p-value: {p_hsie:.2e}\\n\")\n",
        "\n",
        "\n",
        "except RuntimeError:\n",
        "    print(\"Could not fit H'_sie curve.\")\n",
        "    y_fit_hsie, r2_hsie, p_hsie, params_hsie = None, 0, 1, [0,0]\n",
        "\n",
        "# --- 7. Visualize Results ---\n",
        "sns.set_style(\"whitegrid\")\n",
        "fig, axes = plt.subplots(2, 3, figsize=(24, 12))\n",
        "fig.suptitle('Unified Analysis of Learning Dynamics (with Curve Fitting)', fontsize=20)\n",
        "\n",
        "# Column 1: Loss and IB Metrics vs Time\n",
        "axes[0, 0].plot(history['loss'], color='red')\n",
        "axes[0, 0].set_title('Training Loss vs. Epochs')\n",
        "axes[0, 0].set_xlabel('Epoch'); axes[0, 0].set_ylabel('Cross-Entropy Loss')\n",
        "\n",
        "axes[1, 0].plot(log_epochs_np, mi_xt_h1, label='I(X; T)', marker='.')\n",
        "axes[1, 0].plot(log_epochs_np, mi_ty_h1, label='I(T; Y)', marker='.')\n",
        "axes[1, 0].set_title('IB Metrics vs. Epochs')\n",
        "axes[1, 0].set_xlabel('Epoch'); axes[1, 0].set_ylabel('Mutual Information (bits)'); axes[1, 0].legend()\n",
        "\n",
        "# Column 2: Information Plane and Unified Theory Cost vs Time\n",
        "points1 = axes[0, 1].scatter(mi_xt_h1, mi_ty_h1, c=log_epochs_np, cmap='viridis', s=15, alpha=0.8)\n",
        "cbar1 = fig.colorbar(points1, ax=axes[0, 1]); cbar1.set_label('Epoch')\n",
        "axes[0, 1].set_title('The Information Plane (IB Theory)')\n",
        "axes[0, 1].set_xlabel('I(X; T) - Compression'); axes[0, 1].set_ylabel('I(T; Y) - Fitting')\n",
        "\n",
        "axes[1, 1].scatter(log_epochs_np, htse_hist, label=\"H'_tse (Actual Data)\", marker='.', color='green', alpha=0.5)\n",
        "if y_fit_htse is not None:\n",
        "    axes[1, 1].plot(log_epochs_np, y_fit_htse, label=\"Logarithmic Fit\", color='black', linestyle='--')\n",
        "    # NEW: Add the fitted equation to the plot text\n",
        "    fit_eq_htse = f\"$y = {params_htse[0]:.4f} \\\\cdot \\\\ln(x+1) + {params_htse[1]:.4f}$\"\n",
        "    axes[1, 1].text(0.95, 0.05, f'{fit_eq_htse}\\n$R^2 = {r2_htse:.4f}$\\n$p = {p_htse:.2e}$',\n",
        "                    transform=axes[1, 1].transAxes, fontsize=12,\n",
        "                    verticalalignment='bottom', horizontalalignment='right',\n",
        "                    bbox=dict(boxstyle='round,pad=0.5', fc='wheat', alpha=0.5))\n",
        "axes[1, 1].set_title(\"H'_tse (Cognitive Cost) vs. Epochs\")\n",
        "axes[1, 1].set_xlabel('Epoch'); axes[1, 1].set_ylabel(\"H'_tse\"); axes[1, 1].legend()\n",
        "\n",
        "# Column 3: Unified Theory Space and Robustness vs Time\n",
        "points2 = axes[0, 2].scatter(htse_hist, hsie_hist, c=log_epochs_np, cmap='magma', s=15, alpha=0.8)\n",
        "cbar2 = fig.colorbar(points2, ax=axes[0, 2]); cbar2.set_label('Epoch')\n",
        "axes[0, 2].set_title('Weighted Semantic State Space (Unified Theory)')\n",
        "axes[0, 2].set_xlabel(\"H'_tse - Cognitive Cost\"); axes[0, 2].set_ylabel(\"H'_sie - Robustness\")\n",
        "\n",
        "axes[1, 2].scatter(log_epochs_np, hsie_hist, label=\"H'_sie (Actual Data)\", marker='.', color='purple', alpha=0.5)\n",
        "if y_fit_hsie is not None:\n",
        "    axes[1, 2].plot(log_epochs_np, y_fit_hsie, label=\"Logarithmic Fit\", color='black', linestyle='--')\n",
        "    # NEW: Add the fitted equation to the plot text\n",
        "    fit_eq_hsie = f\"$y = -{params_hsie[0]:.4f} \\\\cdot \\\\ln(x+1) + {params_hsie[1]:.4f}$\"\n",
        "    axes[1, 2].text(0.95, 0.95, f'{fit_eq_hsie}\\n$R^2 = {r2_hsie:.4f}$\\n$p = {p_hsie:.2e}$',\n",
        "                    transform=axes[1, 2].transAxes, fontsize=12,\n",
        "                    verticalalignment='top', horizontalalignment='right',\n",
        "                    bbox=dict(boxstyle='round,pad=0.5', fc='wheat', alpha=0.5))\n",
        "axes[1, 2].set_title(\"H'_sie (Robustness) vs. Epochs\")\n",
        "axes[1, 2].set_xlabel('Epoch'); axes[1, 2].set_ylabel(\"H'_sie\"); axes[1, 2].legend()\n",
        "\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
