{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Universality Verification Experiment for Scaling Law (N-Scaling)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset on the first run\n",
    "from torchvision import datasets, transforms\n",
    "try:\n",
    "    datasets.FashionMNIST('./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "    datasets.FashionMNIST('./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "    print(\"FashionMNIST dataset is ready.\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not download dataset. Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the experiment's directory to the Python path\n",
    "workspace_path = \"/N-Scaling\" # Please adjust this path according to your setup\n",
    "if workspace_path not in sys.path:\n",
    "    sys.path.append(workspace_path)\n",
    "    print(f\"Added path: {workspace_path}\")\n",
    "else:\n",
    "    print(f\"Path {workspace_path} is already in the Python path\")\n",
    "\n",
    "# Confirm the existence of the logic module file\n",
    "file_path = os.path.join(workspace_path, \"MLP_N_logic.py\") # Find and replace with the logic module for the desired model\n",
    "if os.path.exists(file_path):\n",
    "    print(f\"✓ File exists: {file_path}\")\n",
    "else:\n",
    "    print(f\"✗ File not found: {file_path}\")\n",
    "\n",
    "# Attempt to import the module\n",
    "try:\n",
    "    import MLP_N_logic # Replace with the logic module for the desired model\n",
    "    print(\"✓ Module imported successfully!\")\n",
    "    \n",
    "    # Check for the presence of the necessary function\n",
    "    if hasattr(MLP_N_logic, 'run_training_task_N_scaling'):\n",
    "        print(\"✓ Found function: run_training_task_N_scaling\")\n",
    "        run_training_task_N_scaling = MLP_N_logic.run_training_task_N_scaling\n",
    "    else:\n",
    "        print(\"✗ Function not found: run_training_task_N_scaling\")\n",
    "        \n",
    "except ImportError as e:\n",
    "    print(f\"✗ Import failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n_scaling_config_cell_v7"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# --- 1. N-Scaling Experiment Configuration ---\n",
    "N_SCALING_CONFIG = {\n",
    "    \"dataset_size\": 10000,\n",
    "    \n",
    "    # Range and number of target parameter counts N\n",
    "    \"target_N_range\": (800, 35000),\n",
    "    \"num_models\": 25,\n",
    "    \n",
    "    \"fixed_epochs\": 80,\n",
    "    \"seed\": 42,\n",
    "}\n",
    "\n",
    "# --- 2. Base Experiment Parameters ---\n",
    "BASE_CONFIG = {\n",
    "    \"batch_size\": 128,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"analysis_sample_size\": 30, \n",
    "    \"w1\": 0.5, \n",
    "    \"w2\": 0.5,\n",
    "}\n",
    "BASE_CONFIG['epochs'] = N_SCALING_CONFIG['fixed_epochs']\n",
    "BASE_CONFIG['dataset_size'] = N_SCALING_CONFIG['dataset_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "main_notebook_cell_n_scaling_v7_3"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import linregress\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from MLP_N_logic import run_training_task_N_scaling, TheoryAnalyzer # Replace with the logic module for the desired model\n",
    "\n",
    "def generate_hidden_configs(target_range, num_points):\n",
    "    \"\"\"Intelligently generate unique (h1, h2) architecture configurations based on the target N range.\"\"\"\n",
    "    target_n_values = np.logspace(np.log10(target_range[0]), np.log10(target_range[1]), num_points)\n",
    "    hidden_size_configs = []\n",
    "    seen_configs = set()\n",
    "    for target_n in target_n_values:\n",
    "        # This is a simplified inverse function to find h1, h2 for a target N\n",
    "        a, b, c = 0.5, 789, -target_n\n",
    "        h1 = (-b + np.sqrt(b**2 - 4*a*c)) / (2*a)\n",
    "        h1 = max(1, int(round(h1)))\n",
    "        h2 = max(1, int(round(h1 / 2)))\n",
    "        config_tuple = (h1, h2)\n",
    "        if config_tuple not in seen_configs:\n",
    "            hidden_size_configs.append(config_tuple)\n",
    "            seen_configs.add(config_tuple)\n",
    "    return hidden_size_configs\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    seed = N_SCALING_CONFIG['seed']\n",
    "    hidden_configs = generate_hidden_configs(N_SCALING_CONFIG['target_N_range'], N_SCALING_CONFIG['num_models'])\n",
    "    \n",
    "    print(f\"Generated {len(hidden_configs)} unique model configurations to test.\")\n",
    "    \n",
    "    tasks = [(seed, h_config, BASE_CONFIG, 0 if torch.cuda.is_available() else -1) for h_config in hidden_configs]\n",
    "    \n",
    "    results = []\n",
    "    for task_args in tqdm(tasks, desc=f\"Running Direct N-Scaling for Seed {seed}\"):\n",
    "        result = run_training_task_N_scaling(task_args)\n",
    "        if result: results.append(result)\n",
    "    \n",
    "    if not results:\n",
    "        print(\"No results generated.\")\n",
    "    else:\n",
    "        df = pd.DataFrame(results).sort_values('num_params_N')\n",
    "        df = df.drop_duplicates(subset=['num_params_N']).reset_index(drop=True)\n",
    "        \n",
    "        # Define the \"valid regime\" as the data points up to the minimum test loss\n",
    "        min_loss_idx = df['final_test_loss'].idxmin()\n",
    "        df_valid = df.loc[:min_loss_idx].copy()\n",
    "        \n",
    "        plt.style.use('seaborn-v0_8-whitegrid')\n",
    "        fig, axes = plt.subplots(1, 5, figsize=(40, 8))\n",
    "        fig.suptitle(f'Cognitive Investment Model - N-Scaling (D={N_SCALING_CONFIG[\"dataset_size\"]}) (Seed={seed}, Epochs={BASE_CONFIG[\"epochs\"]})', fontsize=24, y=0.98)\n",
    "        \n",
    "        # --- Fitting Functions ---\n",
    "        def power_law_fit(x, y):\n",
    "            mask = (y > 0) & (x > 0) & np.isfinite(y) & np.isfinite(x)\n",
    "            if mask.sum() < 2: return 0, 1, 0, np.full_like(x, np.nan, dtype=float)\n",
    "            lx, ly = np.log10(x[mask]), np.log10(y[mask])\n",
    "            s, i, r, p, _ = linregress(lx, ly); r2 = r**2\n",
    "            return r2, p, s, 10**(s*np.log10(x)+i)\n",
    "        \n",
    "        def logarithmic_fit(x, y):\n",
    "            mask = (x > 0) & np.isfinite(y) & np.isfinite(x)\n",
    "            if mask.sum() < 2: return 0, 1, 0, 0, np.full_like(x, np.nan, dtype=float)\n",
    "            log_x = np.log10(x[mask])\n",
    "            y_masked = y[mask]\n",
    "            slope, intercept, r_value, p_value, _ = linregress(log_x, y_masked)\n",
    "            r_squared = r_value**2\n",
    "            y_fit = slope * np.log10(x) + intercept\n",
    "            return r_squared, p_value, slope, intercept, y_fit\n",
    "\n",
    "        # --- Data Preparation for Reducible Metrics ---\n",
    "        L_inf = df_valid['final_test_loss'].min()\n",
    "        df_valid['reducible_loss'] = df_valid['final_test_loss'] - L_inf\n",
    "        hsie_0 = df_valid['final_hsie'].min() # For N-Scaling, H'sie grows from a baseline H_0\n",
    "        df_valid['reducible_hsie'] = df_valid['final_hsie'] - hsie_0\n",
    "        htse_0 = df_valid['final_htse'].min() # H'tse also has a baseline H_0\n",
    "        df_valid['reducible_htse'] = df_valid['final_htse'] - htse_0\n",
    "\n",
    "        x_N_valid = df_valid['num_params_N'].values\n",
    "        \n",
    "        # --- Plot 1: Performance vs. N ---\n",
    "        ax0 = axes[0]\n",
    "        r2_1, p_1, s_1, fit_1 = power_law_fit(x_N_valid, df_valid['reducible_loss'])\n",
    "        ax0.set_title('Law 1: Performance vs. N')\n",
    "        ax0.plot(df['num_params_N'], df['final_test_loss'], 'o', color='blue', label='All Data') # Show all points\n",
    "        ax0.plot(df_valid['num_params_N'], df_valid['final_test_loss'], 'o', color='dodgerblue', label='Valid Regime') # Highlight valid regime\n",
    "        ax0.plot(x_N_valid, fit_1 + L_inf, '--', color='red', label='Fit on Valid Regime')\n",
    "        ax0.text(0.95, 0.95, f'$R^2={r2_1:.2f}, p={p_1:.1e}$\\n$L-L_\\infty \\propto N^{{{s_1:.2f}}}$', ha='right', va='top', transform=ax0.transAxes, bbox=dict(fc='wheat', alpha=0.5))\n",
    "        ax0.set_ylabel('Final Test Loss', fontsize=12)\n",
    "        ax0.legend()\n",
    "\n",
    "        # --- Plot 2: Abstraction ---\n",
    "        r2_2, p_2, s_2, fit_2 = power_law_fit(x_N_valid, df_valid['reducible_htse'])\n",
    "        axes[1].set_title('Component 1: Abstraction')\n",
    "        axes[1].plot(x_N_valid, df_valid['final_htse'], 's', color='green')\n",
    "        axes[1].plot(x_N_valid, fit_2 + htse_0, '--', color='purple')\n",
    "        axes[1].text(0.95, 0.05, f'$R^2={r2_2:.2f}, p={p_2:.1e}$\\n$H-H_0 \\propto N^{{{s_2:.2f}}}$', ha='right', va='bottom', transform=axes[1].transAxes, bbox=dict(fc='wheat', alpha=0.5))\n",
    "        axes[1].set_ylabel(\"Final H'_TSE\", fontsize=12)\n",
    "        \n",
    "        # --- Plot 3: Compression (LOGARITHMIC FIT) ---\n",
    "        r2_3, p_3, a_3, b_3, fit_3 = logarithmic_fit(x_N_valid, df_valid['reducible_hsie'])\n",
    "        axes[2].set_title('Component 2: Compression')\n",
    "        axes[2].plot(x_N_valid, df_valid['final_hsie'], '^', color='orange')\n",
    "        axes[2].plot(x_N_valid, fit_3 + hsie_0, '--', color='darkcyan')\n",
    "        text_3 = (f'$R^2={r2_3:.3f}, p={p_3:.1e}$\\n' f'$H-H_0 \\propto \\log(N)$')\n",
    "        axes[2].text(0.95, 0.95, text_3, ha='right', va='top', transform=axes[2].transAxes, bbox=dict(fc='wheat', alpha=0.5))\n",
    "        axes[2].set_ylabel(\"Final H'_SIE\", fontsize=12)\n",
    "        \n",
    "        # --- Plot 4: Internal Cost Trend ---\n",
    "        htse_red_sq = np.maximum(0, df_valid['reducible_htse'])**2\n",
    "        hsie_red_sq = np.maximum(0, df_valid['reducible_hsie'])**2\n",
    "        df_valid['L_ideal_reducible'] = np.sqrt(BASE_CONFIG['w1'] * htse_red_sq + BASE_CONFIG['w2'] * hsie_red_sq)\n",
    "        axes[3].plot(x_N_valid, df_valid['L_ideal_reducible'], 'p', color='purple')\n",
    "        axes[3].set_title('Internal Cost $\\mathcal{L}_{ideal, red}$ Trend')\n",
    "        axes[3].set_ylabel('Reducible Ideal Norm', fontsize=12)\n",
    "\n",
    "        # --- Plot 5: The Core Law ---\n",
    "        r2_4, p_4, s_4, fit_4 = power_law_fit(df_valid['L_ideal_reducible'], df_valid['reducible_loss'])\n",
    "        axes[4].set_title('The Core Law: Performance vs. Cost')\n",
    "        axes[4].plot(df_valid['L_ideal_reducible'], df_valid['reducible_loss'], 'd', color='black')\n",
    "        axes[4].plot(df_valid['L_ideal_reducible'], fit_4, '--', color='magenta')\n",
    "        axes[4].text(0.95, 0.95, f'Core Law:\\n$R^2={r2_4:.4f}, p={p_4:.1e}$\\n$L_{{red}} \\propto \\mathcal{{L}}_{{ideal, red}}^{{{s_4:.2f}}}$', ha='right', va='top', transform=axes[4].transAxes, bbox=dict(fc='wheat', alpha=0.5))\n",
    "        axes[4].set_ylabel('Reducible Test Loss', fontsize=12)\n",
    "\n",
    "        # --- Final Formatting ---\n",
    "        for i in range(5):\n",
    "            if i < 4:\n",
    "                axes[i].set_xlabel('Number of Parameters (N)')\n",
    "            else:\n",
    "                axes[i].set_xlabel('Reducible Ideal Norm $\\mathcal{L}_{ideal, red}$')\n",
    "            axes[i].set_xscale('log')\n",
    "            # For N-Scaling, the log-growth of H'sie is best visualized on a linear y-axis\n",
    "            if i == 2:\n",
    "                axes[i].set_yscale('linear')\n",
    "            else:\n",
    "                axes[i].set_yscale('log')\n",
    "            axes[i].grid(True, which='both', linestyle='--')\n",
    "        \n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        \n",
    "        # --- Save results ---\n",
    "        output_img_path = f\"N_scaling_D{N_SCALING_CONFIG['dataset_size']}_seed_{seed}_epochs_{BASE_CONFIG['epochs']}.png\"\n",
    "        output_csv_path = f\"N_scaling_D{N_SCALING_CONFIG['dataset_size']}_results_seed_{seed}_epochs_{BASE_CONFIG['epochs']}.csv\"\n",
    "        \n",
    "        plt.savefig(output_img_path, dpi=150)\n",
    "        df.to_csv(output_csv_path, index=False)\n",
    "        \n",
    "        print(f\"Plot saved to: {output_img_path}\")\n",
    "        print(f\"Full experiment data saved to: {output_csv_path}\")\n",
    "        \n",
    "        plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
